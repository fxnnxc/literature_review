# 2023-01
3  * 30 papers 

1. Robustness in deep learning: The good (width), the bad (depth), and the ugly (initialization)
2. ✅ Locating and Editing Factual Associations in GPT (2023.01.02)
3. S3GC: Scalable Self-Supervised Graph Clustering 
4. ✅ Subspace clustering in high-dimensions: Phase transitions \& Statistical-to-Computational gap (2023.01.02)
5. ✅  Learning (Very) Simple Generative Models Is Hard (2023.01.02)
6. Learning to Follow Instructions in Text-Based Games
7. A Pattern Discovery Approach to Multivariate Time Series Forecasting
8. Learning to Share in Multi-Agent Reinforcement Learning
9. Rapidly Mixing Multiple-try Metropolis Algorithms for Model Selection Problems
10. Self-Supervised Learning with an Information Maximization Criterion
11. Off-Team Learning
12. Quantifying Statistical Significance of Neural Network-based Image Segmentation by Selective Inference
13. A Theoretical Understanding of Gradient Bias in Meta-Reinforcement Learning
14. Analyzing Sharpness along GD Trajectory: Progressive Sharpening and Edge of Stability
15. Conformal Frequency Estimation with Sketched Data
16. Decision Trees with Short Explainable Rules
17. Self-Consistent Dynamical Field Theory of Kernel Evolution in Wide Neural Networks
18. Estimating and Explaining Model Performance When Both Covariates and Labels Shift
19. VICE: Variational Interpretable Concept Embeddings
20. Learning to Branch with Tree MDPs
21. SageMix: Saliency-Guided Mixup for Point Clouds
22. Wasserstein Logistic Regression with Mixed Features
23. On Embeddings for Numerical Features in Tabular Deep Learning
24. Towards Understanding Grokking: An Effective Theory of Representation Learning
25. Temporally Disentangled Representation Learning
26. Data-Driven Model-Based Optimization via Invariant Representation Learning
27. Poisson Flow Generative Models
28. On Translation and Reconstruction Guarantees of the Cycle-Consistent Generative Adversarial Networks
29. Chroma-VAE: Mitigating Shortcut Learning with Generative Classifiers
30. [Re] Reproducibility Study of “Counterfactual Generative Networks
31. FedAvg with Fine Tuning: Local Updates Lead to Representation Learning