
# Literature Review 
# Representation Learning in Reinforcement Learning

## NIPS

### 2022ðŸ”–

1. DOMINO: Decomposed Mutual Information Optimization for Generalized Context in Meta-Reinforcement Learning
2. Does Self-supervised Learning Really Improve Reinforcement Learning from Pixels?
3. Explaining a Reinforcement Learning Agent via Prototyping
4. Pre-Trained Image Encoder for Generalizable Visual Reinforcement Learning
5. A Mixture Of Surprises for Unsupervised Reinforcement Learning
6. Spectrum Random Masking for Generalization in Image-based Reinforcement Learning
7. S2P: State-conditioned Image Synthesis for Data Augmentation in Offline Reinforcement Learning
8. Learning Representations via a Robust Behavioral Metric for Deep Reinforcement Learning
9. On the Effect of Pre-training for Transformer in Different Modality on Offline Reinforcement Learning
10. Provable Benefit of Multitask Representation Learning in Reinforcement Learning
11. Explainable Reinforcement Learning via Model Transforms
12. Discrete Compositional Representations as an Abstraction for Goal Conditioned Reinforcement Learning
13. Robust Reinforcement Learning using Offline Data
14. Unsupervised Reinforcement Learning with Contrastive Intrinsic Control

### 2021

15. Widening the Pipeline in Human-Guided Reinforcement Learning with Explanation and Context-Aware Data Augmentation
16. Design of Experiments for Stochastic Contextual Linear Bandits
17. Off-Policy Risk Assessment in Contextual Bandits
18. Federated Linear Contextual Bandits
19. Environment Generation for Zero-Shot Compositional Reinforcement Learning
20. Learning Markov State Abstractions for Deep Reinforcement Learning
21. Towards Deeper Deep Reinforcement Learning with Spectral Normalization
22. Learning MDPs from Features: Predict-Then-Optimize for Sequential Decision Making by Reinforcement Learning
23. Pretraining Representations for Data-Efficient Reinforcement Learning
24. Agnostic Reinforcement Learning with Low-Rank MDPs and Rich Observations
25. Functional Regularization for Reinforcement Learning via Learned Fourier Features
26. Agent Modelling under Partial Observability for Deep Reinforcement Learning
27. Improving Computational Efficiency in Visual Reinforcement Learning via Stored Embeddings
28. Successor Feature Landmarks for Long-Horizon Goal-Conditioned Reinforcement Learning
29. Unsupervised Domain Adaptation with Dynamics-Aware Rewards in Reinforcement Learning


### 2020

30. Reinforcement Learning with Augmented Data
31. Is Plug-in Solver Sample-Efficient for Feature-based Reinforcement Learning?
32. Robust Reinforcement Learning via Adversarial training with Langevin Dynamics
33. Robust Deep Reinforcement Learning against Adversarial Perturbations on State Observations


## ICLR

### 2022

34. Provably Filtering Exogenous Distractors using Multistep Inverse Dynamics
35. The Information Geometry of Unsupervised Reinforcement Learning
36. Learning transferable motor skills with hierarchical latent mixture policies
37. AdaRL: What, Where, and How to Adapt in Transfer Reinforcement Learning
38. On Lottery Tickets and Minimal Task Representations in Deep Reinforcement Learning
39. CoBERL: Contrastive BERT for Reinforcement Learning
40. Learning State Representations via Retracing in Reinforcement Learning
41. Learning Generalizable Representations for Reinforcement Learning via Adaptive Meta-learner of Behavioral Similarities
42. Task-Induced Representation Learning
43. Cross-Trajectory Representation Learning for Zero-Shot Generalization in RL

### 2021

44. Control-Aware Representations for Model-based Reinforcement Learning
45. Contrastive Behavioral Similarity Embeddings for Generalization in Reinforcement Learning
46. Representation Balancing Offline Model-based Reinforcement Learning
47. Off-Dynamics Reinforcement Learning: Training for Transfer with Domain Classifiers
48. Learning Invariant Representations for Reinforcement Learning without Reconstruction
49. Data-Efficient Reinforcement Learning with Self-Predictive Representations
50. Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels
51. Return-Based Contrastive Representation Learning for Reinforcement Learning

### 2020

52. Dynamics-Aware Embeddings
53. Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?
54. DREAM TO CONTROL: LEARNING BEHAVIORS BY LATENT IMAGINATION

## ICML

### 2022

55. DreamerPro: Reconstruction-Free Model-Based Reinforcement Learning with Prototypical Representations
56. Robust Task Representations for Offline Meta-Reinforcement Learning via Contrastive Learning
57. EQR: EQUIVARIANT REPRESENTATIONS FOR DATA-EFFICIENT REINFORCEMENT LEARNING

### 2021
58. Reinforcement Learning with Prototypical Representations
59. RRL: Resnet as representation for Reinforcement Learning
60. Towards Better Laplacian Representation in Reinforcement Learning with Generalized Graph Drawing
61. Decoupling Representation Learning from Reinforcement Learning
62. Multi-Task Reinforcement Learning with Context-based Representations
63. Variational Empowerment as Representation Learning for Goal-Conditioned Reinforcement Learning
64. A Deep Reinforcement Learning Approach to Marginalized Importance Sampling with the Successor Representation


### 2020
65. CURL: Contrastive Unsupervised Representations for Reinforcement Learning
66. Representations for Stable Off-Policy Reinforcement Learning
67. Bootstrap Latent-Predictive Representations for Multitask Reinforcement Learning
68. Skew-Fit: State-Covering Self-Supervised Reinforcement Learning

## AAAI

### 2022

69. Fast and Data Efficient Reinforcement Learning from Pixels via Non-Parametric Value Approximation
70. Same State, Different Task: Continual Reinforcement Learning without Interference
71. Enforcement Heuristics for Argumentation with Deep Reinforcement Learning
72. CADRE: A Cascade Deep Reinforcement Learning Framework for Vision-Based Autonomous Urban Driving
73. Structure Learning-Based Task Decomposition for Reinforcement Learning In Non-Stationary Environments
74. Generalizing Reinforcement Learning through Fusing Self-Supervised Learning into Intrinsic Motivation
75. Unsupervised Reinforcement Learning in Multiple Environments
76. Introducing Symmetries to Black Box Meta Reinforcement Learning
77. Wasserstein Unsupervised Reinforcement Learning
78. Learning by Competition of Self-Interested Reinforcement Learning Agents
79. SimSR: Simple Distance-Based State Representations for Deep Reinforcement Learning
80. Blockwise Sequential Model Learning for Partially Observable Reinforcement Learning
81. Deep Reinforcement Learning Policies Learn Shared Adversarial Features Across MDPs


### 2021

82. Self-Supervised Attention-Aware Reinforcement Learning 
83. Sequential Generative Exploration Model for Partially Observable Reinforcement Learning
84. Towards Effective Context for Meta-Reinforcement Learning: An Approach Based on Contrastive Learning
85. Improving Sample Efficiency in Model-Free Reinforcement Learning from Images
86. Domain Adaptation in Reinforcement Learning via Latent Unified State Representation
87. Reinforcement Learning with a Disentangled Universal Value Function for Item Recommendation
88. The Value-Improvement Path: Towards Better Representations for Reinforcement Learning
89. Uncertainty-Aware Multi-View Representation Learning
90. Distilling Localization for Self-Supervised Representation Learning
91. MERL: Multimodal Event Representation Learning in Heterogeneous Embedding Spaces



## IJCAI

### 2022


92. Feature and Instance Joint Selection: A Reinforcement Learning Perspective
93. Self-Predictive Dynamics for Generalization of Vision-based Reinforcement Learning
94. Donâ€™t Touch What Matters: Task-Aware Lipschitz Data Augmentation for Visual Reinforcement Learning

### 2021

95. Deep Reinforcement Learning Boosted Partial Domain Adaptation


### 2020

96. Efficient Deep Reinforcement Learning via Adaptive Policy Transfer
97. I4R: Promoting Deep Reinforcement Learning by the Indicator for Expressive Representations
98. KoGuN: Accelerating Deep Reinforcement Learning via Integrating Human Suboptimal Knowledge



---
# Comments 

### 1. One-two Settings 

Most of work are done with one-two stage setting. In the first stage representation learning is done and then the RL training proceeds.

[12, 13, 16]

### 2. TestTest
